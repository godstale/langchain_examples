from typing import Iterable
from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableGenerator
from langchain_core.messages import AIMessageChunk

# 1. ëŒ€í™”ë‚´ìš© ì €ì¥ì„ ìœ„í•œ ChatPromptTemplate ì„¤ì •
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ê¸°ìƒí•™ìì…ë‹ˆë‹¤. ë‹µë³€ì€ 200ì ì´ë‚´ë¡œ í•˜ì„¸ìš”."),
    MessagesPlaceholder("chat_history"), # 1-1. í”„ë¡¬í”„íŠ¸ì— ëŒ€í™” ê¸°ë¡ìš© chat_history ì¶”ê°€
    ("user", "{question}")
])

# 2. Ollama ëª¨ë¸ ì´ˆê¸°í™”
llm = Ollama(model="llama3.1")

# 3. ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ íŒŒì„œ ìƒì„± ğŸŒªï¸
def replace_word_with_emoji(text: str) -> str: # ë¬¸ìì—´ì—ì„œ íƒœí’ì„ ì´ëª¨ì§€ë¡œ ë°”ê¿”ì£¼ëŠ” í•¨ìˆ˜
    return text.replace("íƒœí’", "ğŸŒªï¸")

def streaming_parse(chunks: Iterable[AIMessageChunk]) -> Iterable[str]: # type: ignore
    buffer = ""
    for chunk in chunks:
        buffer += chunk
        while " " in buffer: # ì†ë„ê°€ ëŠë¦° ì»´í“¨í„°ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²½ìš° ë‹¨ì–´ê°€ ì™„ì„±ë  ë•Œê¹Œì§€ ëª¨ì•„ì„œ ì²˜ë¦¬
            word, buffer = buffer.split(" ", 1)
            yield replace_word_with_emoji(word) + " "
    if buffer:
        yield replace_word_with_emoji(buffer)

streaming_parser = RunnableGenerator(streaming_parse)

# 4. chain ì—°ê²° (LCEL)
chain = prompt | llm | streaming_parser

# 5. ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
chat_history = []

# 6. chain ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥ì„ ë°˜ë³µ
while True:
    # 6-1. ì‚¬ìš©ìì˜ ì…ë ¥ì„ ê¸°ë‹¤ë¦¼
    user_input = input("\n\në‹¹ì‹ : ")
    if user_input == "ë":
        break

    # 6-2. ì²´ì¸ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ stream í˜•íƒœë¡œ ì¶œë ¥
    result = ""
    for chunk in chain.stream({"question": user_input, "chat_history": chat_history}):
        print(chunk, end="", flush=True)
        result += chunk

    # 6-3. ì±„íŒ… ê¸°ë¡ ì—…ë°ì´íŠ¸
    chat_history.append(("user", user_input))
    chat_history.append(("assistant", result))